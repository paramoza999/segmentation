{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "536f2034-a038-4efb-a773-be2564f1151d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARAMETER ...\n",
      "<__main__.parse_args.<locals>.Args object at 0x15555018e2e0>\n",
      "start loading training data ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:29<00:00,  1.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0872371 1.1862054 1.        1.7367345 2.126663  2.0220127 1.788688\n",
      " 1.8918622 2.0941825 4.048591  1.7390136 2.4426463 1.2522498]\n",
      "Totally 7363 samples in train set.\n",
      "start loading test data ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:04<00:00,  1.73it/s]\n",
      "/home/puoza/p2/Pointnet_Pointnet2_pytorch/data_utils/S3DISDataLoader.py:38: RuntimeWarning: divide by zero encountered in divide\n",
      "  self.labelweights = np.power(np.amax(labelweights) / labelweights, 1 / 3.0)\n",
      "/packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.3699402 1.4154122 1.              inf 7.784936        inf 1.9372996\n",
      " 2.1385393 2.7646968       inf 2.5357857       inf 1.3914901]\n",
      "Totally 1180 samples in test set.\n",
      "The number of training data is: 7363\n",
      "The number of test data is: 1180\n",
      "No existing model, starting training from scratch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Epoch 1 (1/32) ****\n",
      "Learning rate:0.001000\n",
      "BN momentum updated to: 0.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "100%|██████████| 460/460 [09:36<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mean loss: 1.051041\n",
      "Training accuracy: 0.695516\n",
      "Saving at log/sem_seg/pointnet2_sem_seg_hybrid/checkpoints/model.pth\n",
      "Saving model....\n",
      "---- EPOCH 001 EVALUATION ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73/73 [01:02<00:00,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval mean loss: 1.410680\n",
      "eval point avg class IoU: 0.263049\n",
      "eval point accuracy: 0.613322\n",
      "eval point avg class acc: 0.356580\n",
      "------- IoU --------\n",
      "class ceiling        weight: 0.156, IoU: 0.607 \n",
      "class floor          weight: 0.164, IoU: 0.779 \n",
      "class wall           weight: 0.145, IoU: 0.576 \n",
      "class beam           weight: 0.388, IoU: 0.000 \n",
      "class column         weight: 0.000, IoU: 0.000 \n",
      "class window         weight: 0.001, IoU: 0.000 \n",
      "class door           weight: 0.000, IoU: 0.384 \n",
      "class table          weight: 0.058, IoU: 0.433 \n",
      "class chair          weight: 0.043, IoU: 0.268 \n",
      "class sofa           weight: 0.019, IoU: 0.000 \n",
      "class bookcase       weight: 0.000, IoU: 0.172 \n",
      "class board          weight: 0.026, IoU: 0.000 \n",
      "class clutter        weight: 0.000, IoU: 0.200 \n",
      "\n",
      "Eval mean loss: 1.410680\n",
      "Eval accuracy: 0.613322\n",
      "Saving at log/sem_seg/pointnet2_sem_seg_hybrid/checkpoints/best_model.pth\n",
      "Saving model....\n",
      "Best mIoU: 0.263049\n",
      "**** Epoch 2 (2/32) ****\n",
      "Learning rate:0.001000\n",
      "BN momentum updated to: 0.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 460/460 [09:30<00:00,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mean loss: 0.640340\n",
      "Training accuracy: 0.803271\n",
      "---- EPOCH 002 EVALUATION ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 73/73 [01:07<00:00,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval mean loss: 1.375914\n",
      "eval point avg class IoU: 0.278747\n",
      "eval point accuracy: 0.623350\n",
      "eval point avg class acc: 0.376751\n",
      "------- IoU --------\n",
      "class ceiling        weight: 0.157, IoU: 0.604 \n",
      "class floor          weight: 0.168, IoU: 0.834 \n",
      "class wall           weight: 0.149, IoU: 0.564 \n",
      "class beam           weight: 0.386, IoU: 0.000 \n",
      "class column         weight: 0.000, IoU: 0.000 \n",
      "class window         weight: 0.000, IoU: 0.000 \n",
      "class door           weight: 0.000, IoU: 0.474 \n",
      "class table          weight: 0.052, IoU: 0.419 \n",
      "class chair          weight: 0.044, IoU: 0.313 \n",
      "class sofa           weight: 0.021, IoU: 0.000 \n",
      "class bookcase       weight: 0.000, IoU: 0.167 \n",
      "class board          weight: 0.022, IoU: 0.000 \n",
      "class clutter        weight: 0.000, IoU: 0.248 \n",
      "\n",
      "Eval mean loss: 1.375914\n",
      "Eval accuracy: 0.623350\n",
      "Saving at log/sem_seg/pointnet2_sem_seg_hybrid/checkpoints/best_model.pth\n",
      "Saving model....\n",
      "Best mIoU: 0.278747\n",
      "**** Epoch 3 (3/32) ****\n",
      "Learning rate:0.001000\n",
      "BN momentum updated to: 0.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 460/460 [09:20<00:00,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mean loss: 0.508730\n",
      "Training accuracy: 0.841316\n",
      "---- EPOCH 003 EVALUATION ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 73/73 [01:09<00:00,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval mean loss: 1.596352\n",
      "eval point avg class IoU: 0.277098\n",
      "eval point accuracy: 0.622842\n",
      "eval point avg class acc: 0.346505\n",
      "------- IoU --------\n",
      "class ceiling        weight: 0.157, IoU: 0.619 \n",
      "class floor          weight: 0.166, IoU: 0.841 \n",
      "class wall           weight: 0.148, IoU: 0.528 \n",
      "class beam           weight: 0.382, IoU: 0.000 \n",
      "class column         weight: 0.000, IoU: 0.000 \n",
      "class window         weight: 0.001, IoU: 0.000 \n",
      "class door           weight: 0.000, IoU: 0.434 \n",
      "class table          weight: 0.054, IoU: 0.388 \n",
      "class chair          weight: 0.047, IoU: 0.287 \n",
      "class sofa           weight: 0.021, IoU: 0.000 \n",
      "class bookcase       weight: 0.000, IoU: 0.183 \n",
      "class board          weight: 0.024, IoU: 0.000 \n",
      "class clutter        weight: 0.000, IoU: 0.321 \n",
      "\n",
      "Eval mean loss: 1.596352\n",
      "Eval accuracy: 0.622842\n",
      "Best mIoU: 0.278747\n",
      "**** Epoch 4 (4/32) ****\n",
      "Learning rate:0.001000\n",
      "BN momentum updated to: 0.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 460/460 [09:21<00:00,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mean loss: 0.405012\n",
      "Training accuracy: 0.866016\n",
      "---- EPOCH 004 EVALUATION ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 73/73 [01:14<00:00,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval mean loss: 1.579001\n",
      "eval point avg class IoU: 0.276838\n",
      "eval point accuracy: 0.596137\n",
      "eval point avg class acc: 0.371235\n",
      "------- IoU --------\n",
      "class ceiling        weight: 0.158, IoU: 0.615 \n",
      "class floor          weight: 0.173, IoU: 0.849 \n",
      "class wall           weight: 0.151, IoU: 0.460 \n",
      "class beam           weight: 0.374, IoU: 0.000 \n",
      "class column         weight: 0.000, IoU: 0.001 \n",
      "class window         weight: 0.001, IoU: 0.000 \n",
      "class door           weight: 0.000, IoU: 0.456 \n",
      "class table          weight: 0.051, IoU: 0.380 \n",
      "class chair          weight: 0.045, IoU: 0.397 \n",
      "class sofa           weight: 0.022, IoU: 0.000 \n",
      "class bookcase       weight: 0.000, IoU: 0.158 \n",
      "class board          weight: 0.025, IoU: 0.000 \n",
      "class clutter        weight: 0.000, IoU: 0.282 \n",
      "\n",
      "Eval mean loss: 1.579001\n",
      "Eval accuracy: 0.596137\n",
      "Best mIoU: 0.278747\n",
      "**** Epoch 5 (5/32) ****\n",
      "Learning rate:0.001000\n",
      "BN momentum updated to: 0.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 460/460 [09:15<00:00,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mean loss: 0.361628\n",
      "Training accuracy: 0.880149\n",
      "---- EPOCH 005 EVALUATION ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 73/73 [01:15<00:00,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval mean loss: 1.532480\n",
      "eval point avg class IoU: 0.286412\n",
      "eval point accuracy: 0.614397\n",
      "eval point avg class acc: 0.382213\n",
      "------- IoU --------\n",
      "class ceiling        weight: 0.152, IoU: 0.623 \n",
      "class floor          weight: 0.164, IoU: 0.850 \n",
      "class wall           weight: 0.145, IoU: 0.522 \n",
      "class beam           weight: 0.395, IoU: 0.000 \n",
      "class column         weight: 0.000, IoU: 0.028 \n",
      "class window         weight: 0.001, IoU: 0.000 \n",
      "class door           weight: 0.000, IoU: 0.544 \n",
      "class table          weight: 0.054, IoU: 0.334 \n",
      "class chair          weight: 0.045, IoU: 0.371 \n",
      "class sofa           weight: 0.021, IoU: 0.000 \n",
      "class bookcase       weight: 0.000, IoU: 0.173 \n",
      "class board          weight: 0.023, IoU: 0.000 \n",
      "class clutter        weight: 0.000, IoU: 0.281 \n",
      "\n",
      "Eval mean loss: 1.532480\n",
      "Eval accuracy: 0.614397\n",
      "Saving at log/sem_seg/pointnet2_sem_seg_hybrid/checkpoints/best_model.pth\n",
      "Saving model....\n",
      "Best mIoU: 0.286412\n",
      "**** Epoch 6 (6/32) ****\n",
      "Learning rate:0.001000\n",
      "BN momentum updated to: 0.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 460/460 [09:17<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mean loss: 0.312654\n",
      "Training accuracy: 0.894458\n",
      "Saving at log/sem_seg/pointnet2_sem_seg_hybrid/checkpoints/model.pth\n",
      "Saving model....\n",
      "---- EPOCH 006 EVALUATION ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73/73 [01:14<00:00,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval mean loss: 1.464764\n",
      "eval point avg class IoU: 0.286029\n",
      "eval point accuracy: 0.629008\n",
      "eval point avg class acc: 0.379770\n",
      "------- IoU --------\n",
      "class ceiling        weight: 0.145, IoU: 0.608 \n",
      "class floor          weight: 0.165, IoU: 0.839 \n",
      "class wall           weight: 0.147, IoU: 0.548 \n",
      "class beam           weight: 0.390, IoU: 0.000 \n",
      "class column         weight: 0.000, IoU: 0.000 \n",
      "class window         weight: 0.001, IoU: 0.000 \n",
      "class door           weight: 0.000, IoU: 0.565 \n",
      "class table          weight: 0.061, IoU: 0.381 \n",
      "class chair          weight: 0.044, IoU: 0.288 \n",
      "class sofa           weight: 0.021, IoU: 0.000 \n",
      "class bookcase       weight: 0.000, IoU: 0.206 \n",
      "class board          weight: 0.027, IoU: 0.000 \n",
      "class clutter        weight: 0.000, IoU: 0.282 \n",
      "\n",
      "Eval mean loss: 1.464764\n",
      "Eval accuracy: 0.629008\n",
      "Best mIoU: 0.286412\n",
      "**** Epoch 7 (7/32) ****\n",
      "Learning rate:0.001000\n",
      "BN momentum updated to: 0.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 460/460 [09:16<00:00,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mean loss: 0.305693\n",
      "Training accuracy: 0.898862\n",
      "---- EPOCH 007 EVALUATION ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 73/73 [01:14<00:00,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval mean loss: 1.389995\n",
      "eval point avg class IoU: 0.304419\n",
      "eval point accuracy: 0.629468\n",
      "eval point avg class acc: 0.387510\n",
      "------- IoU --------\n",
      "class ceiling        weight: 0.150, IoU: 0.566 \n",
      "class floor          weight: 0.165, IoU: 0.854 \n",
      "class wall           weight: 0.150, IoU: 0.528 \n",
      "class beam           weight: 0.384, IoU: 0.000 \n",
      "class column         weight: 0.000, IoU: 0.001 \n",
      "class window         weight: 0.001, IoU: 0.000 \n",
      "class door           weight: 0.000, IoU: 0.627 \n",
      "class table          weight: 0.060, IoU: 0.459 \n",
      "class chair          weight: 0.045, IoU: 0.362 \n",
      "class sofa           weight: 0.019, IoU: 0.000 \n",
      "class bookcase       weight: 0.000, IoU: 0.223 \n",
      "class board          weight: 0.026, IoU: 0.000 \n",
      "class clutter        weight: 0.000, IoU: 0.337 \n",
      "\n",
      "Eval mean loss: 1.389995\n",
      "Eval accuracy: 0.629468\n",
      "Saving at log/sem_seg/pointnet2_sem_seg_hybrid/checkpoints/best_model.pth\n",
      "Saving model....\n",
      "Best mIoU: 0.304419\n",
      "**** Epoch 8 (8/32) ****\n",
      "Learning rate:0.001000\n",
      "BN momentum updated to: 0.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 460/460 [09:16<00:00,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mean loss: 0.260837\n",
      "Training accuracy: 0.910940\n",
      "---- EPOCH 008 EVALUATION ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 73/73 [01:14<00:00,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval mean loss: 1.526027\n",
      "eval point avg class IoU: 0.278256\n",
      "eval point accuracy: 0.624123\n",
      "eval point avg class acc: 0.388400\n",
      "------- IoU --------\n",
      "class ceiling        weight: 0.152, IoU: 0.584 \n",
      "class floor          weight: 0.167, IoU: 0.817 \n",
      "class wall           weight: 0.148, IoU: 0.570 \n",
      "class beam           weight: 0.385, IoU: 0.000 \n",
      "class column         weight: 0.000, IoU: 0.016 \n",
      "class window         weight: 0.001, IoU: 0.000 \n",
      "class door           weight: 0.000, IoU: 0.449 \n",
      "class table          weight: 0.058, IoU: 0.408 \n",
      "class chair          weight: 0.045, IoU: 0.317 \n",
      "class sofa           weight: 0.019, IoU: 0.000 \n",
      "class bookcase       weight: 0.000, IoU: 0.197 \n",
      "class board          weight: 0.027, IoU: 0.000 \n",
      "class clutter        weight: 0.000, IoU: 0.259 \n",
      "\n",
      "Eval mean loss: 1.526027\n",
      "Eval accuracy: 0.624123\n",
      "Best mIoU: 0.304419\n",
      "**** Epoch 9 (9/32) ****\n",
      "Learning rate:0.001000\n",
      "BN momentum updated to: 0.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 460/460 [09:17<00:00,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mean loss: 0.250473\n",
      "Training accuracy: 0.914773\n",
      "---- EPOCH 009 EVALUATION ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 73/73 [01:14<00:00,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval mean loss: 1.598348\n",
      "eval point avg class IoU: 0.278770\n",
      "eval point accuracy: 0.632347\n",
      "eval point avg class acc: 0.365178\n",
      "------- IoU --------\n",
      "class ceiling        weight: 0.158, IoU: 0.624 \n",
      "class floor          weight: 0.165, IoU: 0.810 \n",
      "class wall           weight: 0.148, IoU: 0.565 \n",
      "class beam           weight: 0.383, IoU: 0.000 \n",
      "class column         weight: 0.000, IoU: 0.001 \n",
      "class window         weight: 0.000, IoU: 0.000 \n",
      "class door           weight: 0.000, IoU: 0.429 \n",
      "class table          weight: 0.058, IoU: 0.410 \n",
      "class chair          weight: 0.042, IoU: 0.339 \n",
      "class sofa           weight: 0.021, IoU: 0.000 \n",
      "class bookcase       weight: 0.000, IoU: 0.138 \n",
      "class board          weight: 0.025, IoU: 0.000 \n",
      "class clutter        weight: 0.000, IoU: 0.308 \n",
      "\n",
      "Eval mean loss: 1.598348\n",
      "Eval accuracy: 0.632347\n",
      "Best mIoU: 0.304419\n",
      "**** Epoch 10 (10/32) ****\n",
      "Learning rate:0.001000\n",
      "BN momentum updated to: 0.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 460/460 [09:16<00:00,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mean loss: 0.233149\n",
      "Training accuracy: 0.920358\n",
      "---- EPOCH 010 EVALUATION ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 73/73 [01:14<00:00,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval mean loss: 1.706574\n",
      "eval point avg class IoU: 0.306600\n",
      "eval point accuracy: 0.673570\n",
      "eval point avg class acc: 0.394000\n",
      "------- IoU --------\n",
      "class ceiling        weight: 0.157, IoU: 0.597 \n",
      "class floor          weight: 0.165, IoU: 0.855 \n",
      "class wall           weight: 0.148, IoU: 0.591 \n",
      "class beam           weight: 0.380, IoU: 0.000 \n",
      "class column         weight: 0.000, IoU: 0.000 \n",
      "class window         weight: 0.001, IoU: 0.000 \n",
      "class door           weight: 0.000, IoU: 0.586 \n",
      "class table          weight: 0.058, IoU: 0.443 \n",
      "class chair          weight: 0.047, IoU: 0.290 \n",
      "class sofa           weight: 0.018, IoU: 0.000 \n",
      "class bookcase       weight: 0.000, IoU: 0.274 \n",
      "class board          weight: 0.025, IoU: 0.000 \n",
      "class clutter        weight: 0.000, IoU: 0.351 \n",
      "\n",
      "Eval mean loss: 1.706574\n",
      "Eval accuracy: 0.673570\n",
      "Saving at log/sem_seg/pointnet2_sem_seg_hybrid/checkpoints/best_model.pth\n",
      "Saving model....\n",
      "Best mIoU: 0.306600\n",
      "**** Epoch 11 (11/32) ****\n",
      "Learning rate:0.000700\n",
      "BN momentum updated to: 0.050000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 460/460 [09:18<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mean loss: 0.193008\n",
      "Training accuracy: 0.932329\n",
      "Saving at log/sem_seg/pointnet2_sem_seg_hybrid/checkpoints/model.pth\n",
      "Saving model....\n",
      "---- EPOCH 011 EVALUATION ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73/73 [01:14<00:00,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval mean loss: 1.710737\n",
      "eval point avg class IoU: 0.276730\n",
      "eval point accuracy: 0.645985\n",
      "eval point avg class acc: 0.392168\n",
      "------- IoU --------\n",
      "class ceiling        weight: 0.157, IoU: 0.550 \n",
      "class floor          weight: 0.166, IoU: 0.804 \n",
      "class wall           weight: 0.150, IoU: 0.595 \n",
      "class beam           weight: 0.392, IoU: 0.000 \n",
      "class column         weight: 0.000, IoU: 0.000 \n",
      "class window         weight: 0.001, IoU: 0.000 \n",
      "class door           weight: 0.000, IoU: 0.520 \n",
      "class table          weight: 0.053, IoU: 0.382 \n",
      "class chair          weight: 0.042, IoU: 0.254 \n",
      "class sofa           weight: 0.018, IoU: 0.000 \n",
      "class bookcase       weight: 0.000, IoU: 0.180 \n",
      "class board          weight: 0.020, IoU: 0.000 \n",
      "class clutter        weight: 0.000, IoU: 0.314 \n",
      "\n",
      "Eval mean loss: 1.710737\n",
      "Eval accuracy: 0.645985\n",
      "Best mIoU: 0.306600\n",
      "**** Epoch 12 (12/32) ****\n",
      "Learning rate:0.000700\n",
      "BN momentum updated to: 0.050000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 460/460 [09:18<00:00,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mean loss: 0.187726\n",
      "Training accuracy: 0.935007\n",
      "---- EPOCH 012 EVALUATION ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 73/73 [01:14<00:00,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval mean loss: 1.827527\n",
      "eval point avg class IoU: 0.264665\n",
      "eval point accuracy: 0.628637\n",
      "eval point avg class acc: 0.358442\n",
      "------- IoU --------\n",
      "class ceiling        weight: 0.154, IoU: 0.532 \n",
      "class floor          weight: 0.165, IoU: 0.801 \n",
      "class wall           weight: 0.145, IoU: 0.563 \n",
      "class beam           weight: 0.388, IoU: 0.000 \n",
      "class column         weight: 0.000, IoU: 0.029 \n",
      "class window         weight: 0.001, IoU: 0.000 \n",
      "class door           weight: 0.000, IoU: 0.556 \n",
      "class table          weight: 0.057, IoU: 0.327 \n",
      "class chair          weight: 0.046, IoU: 0.117 \n",
      "class sofa           weight: 0.019, IoU: 0.000 \n",
      "class bookcase       weight: 0.000, IoU: 0.200 \n",
      "class board          weight: 0.024, IoU: 0.000 \n",
      "class clutter        weight: 0.000, IoU: 0.315 \n",
      "\n",
      "Eval mean loss: 1.827527\n",
      "Eval accuracy: 0.628637\n",
      "Best mIoU: 0.306600\n",
      "**** Epoch 13 (13/32) ****\n",
      "Learning rate:0.000700\n",
      "BN momentum updated to: 0.050000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 460/460 [09:15<00:00,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mean loss: 0.178700\n",
      "Training accuracy: 0.936949\n",
      "---- EPOCH 013 EVALUATION ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 73/73 [01:14<00:00,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval mean loss: 1.700969\n",
      "eval point avg class IoU: 0.286894\n",
      "eval point accuracy: 0.644547\n",
      "eval point avg class acc: 0.378735\n",
      "------- IoU --------\n",
      "class ceiling        weight: 0.153, IoU: 0.486 \n",
      "class floor          weight: 0.167, IoU: 0.833 \n",
      "class wall           weight: 0.151, IoU: 0.618 \n",
      "class beam           weight: 0.367, IoU: 0.000 \n",
      "class column         weight: 0.000, IoU: 0.000 \n",
      "class window         weight: 0.000, IoU: 0.000 \n",
      "class door           weight: 0.000, IoU: 0.524 \n",
      "class table          weight: 0.060, IoU: 0.459 \n",
      "class chair          weight: 0.048, IoU: 0.320 \n",
      "class sofa           weight: 0.023, IoU: 0.000 \n",
      "class bookcase       weight: 0.000, IoU: 0.224 \n",
      "class board          weight: 0.030, IoU: 0.000 \n",
      "class clutter        weight: 0.000, IoU: 0.267 \n",
      "\n",
      "Eval mean loss: 1.700969\n",
      "Eval accuracy: 0.644547\n",
      "Best mIoU: 0.306600\n",
      "**** Epoch 14 (14/32) ****\n",
      "Learning rate:0.000700\n",
      "BN momentum updated to: 0.050000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 460/460 [09:20<00:00,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mean loss: 0.182550\n",
      "Training accuracy: 0.936865\n",
      "---- EPOCH 014 EVALUATION ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 73/73 [01:14<00:00,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval mean loss: 1.671848\n",
      "eval point avg class IoU: 0.293543\n",
      "eval point accuracy: 0.669402\n",
      "eval point avg class acc: 0.379129\n",
      "------- IoU --------\n",
      "class ceiling        weight: 0.158, IoU: 0.599 \n",
      "class floor          weight: 0.164, IoU: 0.848 \n",
      "class wall           weight: 0.143, IoU: 0.651 \n",
      "class beam           weight: 0.391, IoU: 0.000 \n",
      "class column         weight: 0.000, IoU: 0.000 \n",
      "class window         weight: 0.001, IoU: 0.000 \n",
      "class door           weight: 0.000, IoU: 0.601 \n",
      "class table          weight: 0.052, IoU: 0.332 \n",
      "class chair          weight: 0.047, IoU: 0.318 \n",
      "class sofa           weight: 0.019, IoU: 0.000 \n",
      "class bookcase       weight: 0.000, IoU: 0.170 \n",
      "class board          weight: 0.025, IoU: 0.000 \n",
      "class clutter        weight: 0.000, IoU: 0.298 \n",
      "\n",
      "Eval mean loss: 1.671848\n",
      "Eval accuracy: 0.669402\n",
      "Best mIoU: 0.306600\n",
      "**** Epoch 15 (15/32) ****\n",
      "Learning rate:0.000700\n",
      "BN momentum updated to: 0.050000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 460/460 [09:18<00:00,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mean loss: 0.172152\n",
      "Training accuracy: 0.939681\n",
      "---- EPOCH 015 EVALUATION ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 73/73 [01:15<00:00,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval mean loss: 1.568231\n",
      "eval point avg class IoU: 0.310279\n",
      "eval point accuracy: 0.660947\n",
      "eval point avg class acc: 0.408138\n",
      "------- IoU --------\n",
      "class ceiling        weight: 0.155, IoU: 0.557 \n",
      "class floor          weight: 0.167, IoU: 0.839 \n",
      "class wall           weight: 0.151, IoU: 0.581 \n",
      "class beam           weight: 0.375, IoU: 0.000 \n",
      "class column         weight: 0.000, IoU: 0.010 \n",
      "class window         weight: 0.001, IoU: 0.000 \n",
      "class door           weight: 0.000, IoU: 0.644 \n",
      "class table          weight: 0.063, IoU: 0.488 \n",
      "class chair          weight: 0.046, IoU: 0.369 \n",
      "class sofa           weight: 0.018, IoU: 0.000 \n",
      "class bookcase       weight: 0.000, IoU: 0.192 \n",
      "class board          weight: 0.023, IoU: 0.000 \n",
      "class clutter        weight: 0.000, IoU: 0.354 \n",
      "\n",
      "Eval mean loss: 1.568231\n",
      "Eval accuracy: 0.660947\n",
      "Saving at log/sem_seg/pointnet2_sem_seg_hybrid/checkpoints/best_model.pth\n",
      "Saving model....\n",
      "Best mIoU: 0.310279\n",
      "**** Epoch 16 (16/32) ****\n",
      "Learning rate:0.000700\n",
      "BN momentum updated to: 0.050000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 11%|█         | 51/460 [01:10<09:27,  1.39s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 372\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    371\u001b[0m     args \u001b[38;5;241m=\u001b[39m parse_args()\n\u001b[0;32m--> 372\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 264\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    261\u001b[0m points, target \u001b[38;5;241m=\u001b[39m points\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mcuda(), target\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m    262\u001b[0m points \u001b[38;5;241m=\u001b[39m points\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 264\u001b[0m seg_pred, trans_feat \u001b[38;5;241m=\u001b[39m \u001b[43mclassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoints\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m seg_pred \u001b[38;5;241m=\u001b[39m seg_pred\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, NUM_CLASSES)\n\u001b[1;32m    267\u001b[0m batch_label \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)[:, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m/packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/p2/Pointnet_Pointnet2_pytorch/models/pointnet2_sem_seg_hybrid.py:59\u001b[0m, in \u001b[0;36mget_model.forward\u001b[0;34m(self, xyz)\u001b[0m\n\u001b[1;32m     56\u001b[0m l0_points \u001b[38;5;241m=\u001b[39m xyz\n\u001b[1;32m     57\u001b[0m l0_xyz \u001b[38;5;241m=\u001b[39m xyz[:, :\u001b[38;5;241m3\u001b[39m, :]\n\u001b[0;32m---> 59\u001b[0m l1_xyz, l1_points \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msa1\u001b[49m\u001b[43m(\u001b[49m\u001b[43ml0_xyz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml0_points\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m l2_xyz, l2_points \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msa2(l1_xyz, l1_points)\n\u001b[1;32m     61\u001b[0m l3_xyz, l3_points \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msa3(l2_xyz, l2_points)\n",
      "File \u001b[0;32m/packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/p2/Pointnet_Pointnet2_pytorch/models/pointnet2_utils.py:261\u001b[0m, in \u001b[0;36mPointNetSetAbstraction.forward\u001b[0;34m(self, xyz, points)\u001b[0m\n\u001b[1;32m    259\u001b[0m     new_xyz, new_points \u001b[38;5;241m=\u001b[39m sample_and_group_all(xyz, points)\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 261\u001b[0m     new_xyz, new_points \u001b[38;5;241m=\u001b[39m \u001b[43msample_and_group\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mradius\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnsample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxyz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoints\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# new_xyz: sampled points position data, [B, npoint, C]\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# new_points: sampled points data, [B, npoint, nsample, C+D]\u001b[39;00m\n\u001b[1;32m    264\u001b[0m new_points \u001b[38;5;241m=\u001b[39m new_points\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# [B, C+D, nsample,npoint]\u001b[39;00m\n",
      "File \u001b[0;32m~/p2/Pointnet_Pointnet2_pytorch/models/pointnet2_utils.py:124\u001b[0m, in \u001b[0;36msample_and_group\u001b[0;34m(npoint, radius, nsample, xyz, points, returnfps)\u001b[0m\n\u001b[1;32m    122\u001b[0m B, N, C \u001b[38;5;241m=\u001b[39m xyz\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    123\u001b[0m S \u001b[38;5;241m=\u001b[39m npoint\n\u001b[0;32m--> 124\u001b[0m fps_idx \u001b[38;5;241m=\u001b[39m \u001b[43mfarthest_point_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxyz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnpoint\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# [B, npoint, C]\u001b[39;00m\n\u001b[1;32m    125\u001b[0m new_xyz \u001b[38;5;241m=\u001b[39m index_points(xyz, fps_idx)\n\u001b[1;32m    126\u001b[0m idx \u001b[38;5;241m=\u001b[39m query_ball_point(radius, nsample, xyz, new_xyz)\n",
      "File \u001b[0;32m~/p2/Pointnet_Pointnet2_pytorch/models/pointnet2_utils.py:82\u001b[0m, in \u001b[0;36mfarthest_point_sample\u001b[0;34m(xyz, npoint)\u001b[0m\n\u001b[1;32m     80\u001b[0m     dist \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum((xyz \u001b[38;5;241m-\u001b[39m centroid) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     81\u001b[0m     mask \u001b[38;5;241m=\u001b[39m dist \u001b[38;5;241m<\u001b[39m distance\n\u001b[0;32m---> 82\u001b[0m     \u001b[43mdistance\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m dist[mask]\n\u001b[1;32m     83\u001b[0m     farthest \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(distance, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m centroids\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import datetime\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import importlib\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def save_metrics(training_mean_loss, training_accuracy, eval_mean_loss, eval_accuracy, file_path):\n",
    "    with open(file_path, 'a') as f:\n",
    "        f.write(f\"{training_mean_loss[-1]},{training_accuracy[-1]},{eval_mean_loss[-1]},{eval_accuracy[-1]}\\n\")\n",
    "\n",
    "def plot_and_save_metrics(training_mean_loss, training_accuracy, eval_mean_loss, eval_accuracy, experiment_dir):\n",
    "    # Move tensors to CPU\n",
    "    training_mean_loss_cpu = [val.item() for val in training_mean_loss]\n",
    "    training_accuracy_cpu = [val.item() for val in training_accuracy]\n",
    "    eval_mean_loss_cpu = [val.item() for val in eval_mean_loss]\n",
    "    eval_accuracy_cpu = [val.item() for val in eval_accuracy]\n",
    "\n",
    "    # Plot training mean loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(training_mean_loss_cpu, label='Training Mean Loss', color='blue')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Loss')\n",
    "    plt.title('Training Mean Loss per Epoch')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(str(experiment_dir) + '/training_mean_loss.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Plot training accuracy\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(training_accuracy_cpu, label='Training Accuracy', color='green')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training Accuracy per Epoch')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(str(experiment_dir) + '/training_accuracy.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Plot evaluation mean loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(eval_mean_loss_cpu, label='Evaluation Mean Loss', color='red')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Loss')\n",
    "    plt.title('Evaluation Mean Loss per Epoch')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(str(experiment_dir) + '/eval_mean_loss.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Plot evaluation accuracy\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(eval_accuracy_cpu, label='Evaluation Accuracy', color='purple')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Evaluation Accuracy per Epoch')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(str(experiment_dir) + '/eval_accuracy.png')\n",
    "    plt.close()\n",
    "# Usage:\n",
    "# plot_and_save_metrics(training_mean_loss, training_accuracy, eval_mean_loss, eval_accuracy, experiment_dir)\n",
    "\n",
    "\n",
    "\n",
    "# Import the necessary modules here\n",
    "from data_utils.S3DISDataLoader import S3DISDataset\n",
    "import provider\n",
    "\n",
    "# Adjusted BASE_DIR and ROOT_DIR setup\n",
    "BASE_DIR = os.getcwd()  # Use the current working directory\n",
    "ROOT_DIR = BASE_DIR\n",
    "sys.path.append(os.path.join(ROOT_DIR, 'models'))\n",
    "\n",
    "# Your class and function definitions remain unchanged...\n",
    "classes = ['ceiling', 'floor', 'wall', 'beam', 'column', 'window', 'door', 'table', 'chair', 'sofa', 'bookcase',\n",
    "           'board', 'clutter']\n",
    "class2label = {cls: i for i, cls in enumerate(classes)}\n",
    "seg_classes = class2label\n",
    "seg_label_to_cat = {}\n",
    "for i, cat in enumerate(seg_classes.keys()):\n",
    "    seg_label_to_cat[i] = cat\n",
    "\n",
    "def inplace_relu(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('ReLU') != -1:\n",
    "        m.inplace=True\n",
    "\n",
    "def parse_args():\n",
    "    \"\"\"Parse command line arguments or set default values if running in a Jupyter notebook/IPython.\"\"\"\n",
    "    if sys.argv[0].endswith('ipykernel_launcher.py') or 'ipykernel' in sys.argv[0]:\n",
    "        # Directly define args for Jupyter/Colab notebooks or IPython\n",
    "        class Args:\n",
    "            model = 'pointnet2_sem_seg_hybrid'\n",
    "            batch_size = 16\n",
    "            epoch = 32\n",
    "            learning_rate = 0.001\n",
    "            gpu = '0'\n",
    "            optimizer = 'Adam'\n",
    "            log_dir = 'pointnet2_sem_seg_hybrid'\n",
    "            decay_rate = 1e-4\n",
    "            npoint = 4096\n",
    "            step_size = 10\n",
    "            lr_decay = 0.7\n",
    "            test_area = 2\n",
    "        return Args()\n",
    "    else:\n",
    "        # Original argparse code for command-line execution\n",
    "        parser = argparse.ArgumentParser('Model')\n",
    "        parser.add_argument('--model', type=str, default='pointnet_sem_seg', help='model name [default: pointnet_sem_seg]')\n",
    "        parser.add_argument('--batch_size', type=int, default=16, help='Batch Size during training [default: 16]')\n",
    "        # Add the rest of your arguments here\n",
    "        return parser.parse_args()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    def log_string(str):\n",
    "        logger.info(str)\n",
    "        print(str)\n",
    "\n",
    "    '''HYPER PARAMETER'''\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu\n",
    "\n",
    "    '''CREATE DIR'''\n",
    "    timestr = str(datetime.datetime.now().strftime('%Y-%m-%d_%H-%M'))\n",
    "    experiment_dir = Path('./log/')\n",
    "    experiment_dir.mkdir(exist_ok=True)\n",
    "    experiment_dir = experiment_dir.joinpath('sem_seg')\n",
    "    experiment_dir.mkdir(exist_ok=True)\n",
    "    if args.log_dir is None:\n",
    "        experiment_dir = experiment_dir.joinpath(timestr)\n",
    "    else:\n",
    "        experiment_dir = experiment_dir.joinpath(args.log_dir)\n",
    "    experiment_dir.mkdir(exist_ok=True)\n",
    "    checkpoints_dir = experiment_dir.joinpath('checkpoints/')\n",
    "    checkpoints_dir.mkdir(exist_ok=True)\n",
    "    log_dir = experiment_dir.joinpath('logs/')\n",
    "    log_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    '''LOG'''\n",
    "    args = parse_args()\n",
    "    logger = logging.getLogger(\"Model\")\n",
    "    logger.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    file_handler = logging.FileHandler('%s/%s.txt' % (log_dir, args.model))\n",
    "    file_handler.setLevel(logging.INFO)\n",
    "    file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(file_handler)\n",
    "    log_string('PARAMETER ...')\n",
    "    log_string(args)\n",
    "\n",
    "    root = '/home/puoza/p2/Pointnet_Pointnet2_pytorch/new'\n",
    "    NUM_CLASSES = 13\n",
    "    NUM_POINT = args.npoint\n",
    "    BATCH_SIZE = args.batch_size\n",
    "\n",
    "    print(\"start loading training data ...\")\n",
    "    TRAIN_DATASET = S3DISDataset(split='train', data_root=root, num_point=NUM_POINT, test_area=args.test_area, block_size=1.0, sample_rate=1.0, transform=None)\n",
    "    print(\"start loading test data ...\")\n",
    "    TEST_DATASET = S3DISDataset(split='test', data_root=root, num_point=NUM_POINT, test_area=args.test_area, block_size=1.0, sample_rate=1.0, transform=None)\n",
    "\n",
    "    trainDataLoader = torch.utils.data.DataLoader(TRAIN_DATASET, batch_size=BATCH_SIZE, shuffle=True, num_workers=10,\n",
    "                                                  pin_memory=True, drop_last=True,\n",
    "                                                  worker_init_fn=lambda x: np.random.seed(x + int(time.time())))\n",
    "    testDataLoader = torch.utils.data.DataLoader(TEST_DATASET, batch_size=BATCH_SIZE, shuffle=False, num_workers=10,\n",
    "                                                 pin_memory=True, drop_last=True)\n",
    "    weights = torch.Tensor(TRAIN_DATASET.labelweights).cuda()\n",
    "\n",
    "    log_string(\"The number of training data is: %d\" % len(TRAIN_DATASET))\n",
    "    log_string(\"The number of test data is: %d\" % len(TEST_DATASET))\n",
    "\n",
    "    '''MODEL LOADING'''\n",
    "    MODEL = importlib.import_module(args.model)\n",
    "    shutil.copy('models/%s.py' % args.model, str(experiment_dir))\n",
    "    shutil.copy('models/pointnet2_utils.py', str(experiment_dir))\n",
    "\n",
    "    classifier = MODEL.get_model(NUM_CLASSES).cuda()\n",
    "    criterion = MODEL.get_loss().cuda()\n",
    "    classifier.apply(inplace_relu)\n",
    "\n",
    "    def weights_init(m):\n",
    "        classname = m.__class__.__name__\n",
    "        if classname.find('Conv2d') != -1:\n",
    "            torch.nn.init.xavier_normal_(m.weight.data)\n",
    "            torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "        elif classname.find('Linear') != -1:\n",
    "            torch.nn.init.xavier_normal_(m.weight.data)\n",
    "            torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "    try:\n",
    "        checkpoint = torch.load(str(experiment_dir) + '/checkpoints/best_model.pth')\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        classifier.load_state_dict(checkpoint['model_state_dict'])\n",
    "        log_string('Use pretrain model')\n",
    "    except:\n",
    "        log_string('No existing model, starting training from scratch...')\n",
    "        start_epoch = 0\n",
    "        classifier = classifier.apply(weights_init)\n",
    "\n",
    "    if args.optimizer == 'Adam':\n",
    "        optimizer = torch.optim.Adam(\n",
    "            classifier.parameters(),\n",
    "            lr=args.learning_rate,\n",
    "            betas=(0.9, 0.999),\n",
    "            eps=1e-08,\n",
    "            weight_decay=args.decay_rate\n",
    "        )\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD(classifier.parameters(), lr=args.learning_rate, momentum=0.9)\n",
    "\n",
    "    def bn_momentum_adjust(m, momentum):\n",
    "        if isinstance(m, torch.nn.BatchNorm2d) or isinstance(m, torch.nn.BatchNorm1d):\n",
    "            m.momentum = momentum\n",
    "\n",
    "    LEARNING_RATE_CLIP = 1e-5\n",
    "    MOMENTUM_ORIGINAL = 0.1\n",
    "    MOMENTUM_DECCAY = 0.5\n",
    "    MOMENTUM_DECCAY_STEP = args.step_size\n",
    "\n",
    "    global_epoch = 0\n",
    "    best_iou = 0\n",
    "\n",
    "    for epoch in range(start_epoch, args.epoch):\n",
    "        '''Train on chopped scenes'''\n",
    "        log_string('**** Epoch %d (%d/%s) ****' % (global_epoch + 1, epoch + 1, args.epoch))\n",
    "        lr = max(args.learning_rate * (args.lr_decay ** (epoch // args.step_size)), LEARNING_RATE_CLIP)\n",
    "        log_string('Learning rate:%f' % lr)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        momentum = MOMENTUM_ORIGINAL * (MOMENTUM_DECCAY ** (epoch // MOMENTUM_DECCAY_STEP))\n",
    "        if momentum < 0.01:\n",
    "            momentum = 0.01\n",
    "        print('BN momentum updated to: %f' % momentum)\n",
    "        classifier = classifier.apply(lambda x: bn_momentum_adjust(x, momentum))\n",
    "        num_batches = len(trainDataLoader)\n",
    "        total_correct = 0\n",
    "        total_seen = 0\n",
    "        loss_sum = 0\n",
    "        classifier = classifier.train()\n",
    "\n",
    "        for i, (points, target) in tqdm(enumerate(trainDataLoader), total=len(trainDataLoader), smoothing=0.9):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            points = points.data.numpy()\n",
    "            points[:, :, :3] = provider.rotate_point_cloud_z(points[:, :, :3])\n",
    "            points = torch.Tensor(points)\n",
    "            points, target = points.float().cuda(), target.long().cuda()\n",
    "            points = points.transpose(2, 1)\n",
    "\n",
    "            seg_pred, trans_feat = classifier(points)\n",
    "            seg_pred = seg_pred.contiguous().view(-1, NUM_CLASSES)\n",
    "\n",
    "            batch_label = target.view(-1, 1)[:, 0].cpu().data.numpy()\n",
    "            target = target.view(-1, 1)[:, 0]\n",
    "            loss = criterion(seg_pred, target, trans_feat, weights)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            pred_choice = seg_pred.cpu().data.max(1)[1].numpy()\n",
    "            correct = np.sum(pred_choice == batch_label)\n",
    "            total_correct += correct\n",
    "            total_seen += (BATCH_SIZE * NUM_POINT)\n",
    "            loss_sum += loss\n",
    "        log_string('Training mean loss: %f' % (loss_sum / num_batches))\n",
    "        log_string('Training accuracy: %f' % (total_correct / float(total_seen)))\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            logger.info('Save model...')\n",
    "            savepath = str(checkpoints_dir) + '/model.pth'\n",
    "            log_string('Saving at %s' % savepath)\n",
    "            state = {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': classifier.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }\n",
    "            torch.save(state, savepath)\n",
    "            log_string('Saving model....')\n",
    "\n",
    "        '''Evaluate on chopped scenes'''\n",
    "        with torch.no_grad():\n",
    "            num_batches = len(testDataLoader)\n",
    "            total_correct = 0\n",
    "            total_seen = 0\n",
    "            loss_sum = 0\n",
    "            labelweights = np.zeros(NUM_CLASSES)\n",
    "            total_seen_class = [0 for _ in range(NUM_CLASSES)]\n",
    "            total_correct_class = [0 for _ in range(NUM_CLASSES)]\n",
    "            total_iou_deno_class = [0 for _ in range(NUM_CLASSES)]\n",
    "            classifier = classifier.eval()\n",
    "\n",
    "            log_string('---- EPOCH %03d EVALUATION ----' % (global_epoch + 1))\n",
    "            for i, (points, target) in tqdm(enumerate(testDataLoader), total=len(testDataLoader), smoothing=0.9):\n",
    "                points = points.data.numpy()\n",
    "                points = torch.Tensor(points)\n",
    "                points, target = points.float().cuda(), target.long().cuda()\n",
    "                points = points.transpose(2, 1)\n",
    "\n",
    "                seg_pred, trans_feat = classifier(points)\n",
    "                pred_val = seg_pred.contiguous().cpu().data.numpy()\n",
    "                seg_pred = seg_pred.contiguous().view(-1, NUM_CLASSES)\n",
    "\n",
    "                batch_label = target.cpu().data.numpy()\n",
    "                target = target.view(-1, 1)[:, 0]\n",
    "                loss = criterion(seg_pred, target, trans_feat, weights)\n",
    "                loss_sum += loss\n",
    "                pred_val = np.argmax(pred_val, 2)\n",
    "                correct = np.sum((pred_val == batch_label))\n",
    "                total_correct += correct\n",
    "                total_seen += (BATCH_SIZE * NUM_POINT)\n",
    "                tmp, _ = np.histogram(batch_label, range(NUM_CLASSES + 1))\n",
    "                labelweights += tmp\n",
    "\n",
    "                for l in range(NUM_CLASSES):\n",
    "                    total_seen_class[l] += np.sum((batch_label == l))\n",
    "                    total_correct_class[l] += np.sum((pred_val == l) & (batch_label == l))\n",
    "                    total_iou_deno_class[l] += np.sum(((pred_val == l) | (batch_label == l)))\n",
    "\n",
    "            labelweights = labelweights.astype(float) / np.sum(labelweights.astype(float))\n",
    "            mIoU = np.mean(np.array(total_correct_class) / (np.array(total_iou_deno_class, dtype=float) + 1e-6))\n",
    "            log_string('eval mean loss: %f' % (loss_sum / float(num_batches)))\n",
    "            log_string('eval point avg class IoU: %f' % (mIoU))\n",
    "            log_string('eval point accuracy: %f' % (total_correct / float(total_seen)))\n",
    "            log_string('eval point avg class acc: %f' % (\n",
    "                np.mean(np.array(total_correct_class) / (np.array(total_seen_class, dtype=float) + 1e-6))))\n",
    "\n",
    "            iou_per_class_str = '------- IoU --------\\n'\n",
    "            for l in range(NUM_CLASSES):\n",
    "                iou_per_class_str += 'class %s weight: %.3f, IoU: %.3f \\n' % (\n",
    "                    seg_label_to_cat[l] + ' ' * (14 - len(seg_label_to_cat[l])), labelweights[l - 1],\n",
    "                    total_correct_class[l] / float(total_iou_deno_class[l]))\n",
    "\n",
    "            log_string(iou_per_class_str)\n",
    "            log_string('Eval mean loss: %f' % (loss_sum / num_batches))\n",
    "            log_string('Eval accuracy: %f' % (total_correct / float(total_seen)))\n",
    "\n",
    "            if mIoU >= best_iou:\n",
    "                best_iou = mIoU\n",
    "                logger.info('Save model...')\n",
    "                savepath = str(checkpoints_dir) + '/best_model.pth'\n",
    "                log_string('Saving at %s' % savepath)\n",
    "                state = {\n",
    "                    'epoch': epoch,\n",
    "                    'class_avg_iou': mIoU,\n",
    "                    'model_state_dict': classifier.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                }\n",
    "                torch.save(state, savepath)\n",
    "                log_string('Saving model....')\n",
    "            log_string('Best mIoU: %f' % best_iou)\n",
    "        global_epoch += 1\n",
    "        #plot_and_save_metrics(training_mean_loss, training_accuracy, eval_mean_loss, eval_accuracy, experiment_dir)\n",
    "        #save_metrics(training_mean_loss, training_accuracy, eval_mean_loss, eval_accuracy, experiment_dir)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    args = parse_args()\n",
    "    main(args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e236067d-1299-4534-af72-cb9299bcc04c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu-2.1.0-cuda-12.1",
   "language": "python",
   "name": "pytorch-gpu-2.1.0-cuda-12.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
