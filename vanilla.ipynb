{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e77833-e68a-4391-845b-cf575ecb8a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARAMETER ...\n",
      "<__main__.parse_args.<locals>.Args object at 0x155550142d30>\n",
      "start loading training data ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:02<00:00, 13.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0872371 1.1862054 1.        1.7367345 2.126663  2.0220127 1.788688\n",
      " 1.8918622 2.0941825 4.048591  1.7390136 2.4426463 1.2522498]\n",
      "Totally 7363 samples in train set.\n",
      "start loading test data ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 21.11it/s]\n",
      "/home/puoza/p2/Pointnet_Pointnet2_pytorch/data_utils/S3DISDataLoader.py:38: RuntimeWarning: divide by zero encountered in divide\n",
      "  self.labelweights = np.power(np.amax(labelweights) / labelweights, 1 / 3.0)\n",
      "/packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.3699402 1.4154122 1.              inf 7.784936        inf 1.9372996\n",
      " 2.1385393 2.7646968       inf 2.5357857       inf 1.3914901]\n",
      "Totally 1180 samples in test set.\n",
      "The number of training data is: 7363\n",
      "The number of test data is: 1180\n",
      "Use pretrain model\n",
      "**** Epoch 1 (1/32) ****\n",
      "Learning rate:0.001000\n",
      "BN momentum updated to: 0.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/packages/envs/pytorch-gpu-2.1.0-cuda-12.1/lib/python3.9/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "100%|██████████| 460/460 [04:50<00:00,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mean loss: 0.731074\n",
      "Training accuracy: 0.773577\n",
      "Saving at log/sem_seg/pointnet2_sem_seg_tran/checkpoints/model.pth\n",
      "Saving model....\n",
      "---- EPOCH 001 EVALUATION ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 73/73 [00:32<00:00,  2.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval mean loss: 1.691470\n",
      "eval point avg class IoU: 0.247397\n",
      "eval point accuracy: 0.568435\n",
      "eval point avg class acc: 0.334346\n",
      "------- IoU --------\n",
      "class ceiling        weight: 0.164, IoU: 0.504 \n",
      "class floor          weight: 0.170, IoU: 0.814 \n",
      "class wall           weight: 0.149, IoU: 0.528 \n",
      "class beam           weight: 0.367, IoU: 0.000 \n",
      "class column         weight: 0.000, IoU: 0.000 \n",
      "class window         weight: 0.001, IoU: 0.000 \n",
      "class door           weight: 0.000, IoU: 0.479 \n",
      "class table          weight: 0.056, IoU: 0.340 \n",
      "class chair          weight: 0.047, IoU: 0.238 \n",
      "class sofa           weight: 0.020, IoU: 0.000 \n",
      "class bookcase       weight: 0.000, IoU: 0.111 \n",
      "class board          weight: 0.026, IoU: 0.000 \n",
      "class clutter        weight: 0.000, IoU: 0.204 \n",
      "\n",
      "Eval mean loss: 1.691470\n",
      "Eval accuracy: 0.568435\n",
      "Saving at log/sem_seg/pointnet2_sem_seg_tran/checkpoints/best_model.pth\n",
      "Saving model....\n",
      "Best mIoU: 0.247397\n",
      "**** Epoch 2 (2/32) ****\n",
      "Learning rate:0.001000\n",
      "BN momentum updated to: 0.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 460/460 [04:51<00:00,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mean loss: 0.549866\n",
      "Training accuracy: 0.824319\n",
      "---- EPOCH 002 EVALUATION ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 73/73 [00:32<00:00,  2.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval mean loss: 1.725555\n",
      "eval point avg class IoU: 0.262722\n",
      "eval point accuracy: 0.591020\n",
      "eval point avg class acc: 0.348289\n",
      "------- IoU --------\n",
      "class ceiling        weight: 0.157, IoU: 0.520 \n",
      "class floor          weight: 0.165, IoU: 0.802 \n",
      "class wall           weight: 0.142, IoU: 0.524 \n",
      "class beam           weight: 0.385, IoU: 0.000 \n",
      "class column         weight: 0.000, IoU: 0.000 \n",
      "class window         weight: 0.001, IoU: 0.000 \n",
      "class door           weight: 0.000, IoU: 0.480 \n",
      "class table          weight: 0.055, IoU: 0.420 \n",
      "class chair          weight: 0.043, IoU: 0.257 \n",
      "class sofa           weight: 0.022, IoU: 0.000 \n",
      "class bookcase       weight: 0.000, IoU: 0.145 \n",
      "class board          weight: 0.029, IoU: 0.000 \n",
      "class clutter        weight: 0.000, IoU: 0.267 \n",
      "\n",
      "Eval mean loss: 1.725555\n",
      "Eval accuracy: 0.591020\n",
      "Saving at log/sem_seg/pointnet2_sem_seg_tran/checkpoints/best_model.pth\n",
      "Saving model....\n",
      "Best mIoU: 0.262722\n",
      "**** Epoch 3 (3/32) ****\n",
      "Learning rate:0.001000\n",
      "BN momentum updated to: 0.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 460/460 [04:50<00:00,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mean loss: 0.452486\n",
      "Training accuracy: 0.848800\n",
      "---- EPOCH 003 EVALUATION ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 73/73 [00:32<00:00,  2.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval mean loss: 1.756842\n",
      "eval point avg class IoU: 0.239607\n",
      "eval point accuracy: 0.553503\n",
      "eval point avg class acc: 0.325495\n",
      "------- IoU --------\n",
      "class ceiling        weight: 0.158, IoU: 0.277 \n",
      "class floor          weight: 0.162, IoU: 0.828 \n",
      "class wall           weight: 0.144, IoU: 0.575 \n",
      "class beam           weight: 0.386, IoU: 0.000 \n",
      "class column         weight: 0.000, IoU: 0.000 \n",
      "class window         weight: 0.001, IoU: 0.000 \n",
      "class door           weight: 0.000, IoU: 0.496 \n",
      "class table          weight: 0.058, IoU: 0.330 \n",
      "class chair          weight: 0.044, IoU: 0.255 \n",
      "class sofa           weight: 0.019, IoU: 0.000 \n",
      "class bookcase       weight: 0.000, IoU: 0.148 \n",
      "class board          weight: 0.029, IoU: 0.000 \n",
      "class clutter        weight: 0.000, IoU: 0.206 \n",
      "\n",
      "Eval mean loss: 1.756842\n",
      "Eval accuracy: 0.553503\n",
      "Best mIoU: 0.262722\n",
      "**** Epoch 4 (4/32) ****\n",
      "Learning rate:0.001000\n",
      "BN momentum updated to: 0.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 28%|██▊       | 131/460 [01:27<03:13,  1.70it/s]"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import datetime\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import importlib\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def save_metrics(training_mean_loss, training_accuracy, eval_mean_loss, eval_accuracy, file_path):\n",
    "    with open(file_path, 'a') as f:\n",
    "        f.write(f\"{training_mean_loss[-1]},{training_accuracy[-1]},{eval_mean_loss[-1]},{eval_accuracy[-1]}\\n\")\n",
    "\n",
    "def plot_and_save_metrics(training_mean_loss, training_accuracy, eval_mean_loss, eval_accuracy, experiment_dir):\n",
    "    # Move tensors to CPU\n",
    "    training_mean_loss_cpu = [val.item() for val in training_mean_loss]\n",
    "    training_accuracy_cpu = [val.item() for val in training_accuracy]\n",
    "    eval_mean_loss_cpu = [val.item() for val in eval_mean_loss]\n",
    "    eval_accuracy_cpu = [val.item() for val in eval_accuracy]\n",
    "\n",
    "    # Plot training mean loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(training_mean_loss_cpu, label='Training Mean Loss', color='blue')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Loss')\n",
    "    plt.title('Training Mean Loss per Epoch')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(str(experiment_dir) + '/training_mean_loss.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Plot training accuracy\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(training_accuracy_cpu, label='Training Accuracy', color='green')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training Accuracy per Epoch')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(str(experiment_dir) + '/training_accuracy.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Plot evaluation mean loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(eval_mean_loss_cpu, label='Evaluation Mean Loss', color='red')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Loss')\n",
    "    plt.title('Evaluation Mean Loss per Epoch')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(str(experiment_dir) + '/eval_mean_loss.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Plot evaluation accuracy\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(eval_accuracy_cpu, label='Evaluation Accuracy', color='purple')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Evaluation Accuracy per Epoch')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(str(experiment_dir) + '/eval_accuracy.png')\n",
    "    plt.close()\n",
    "# Usage:\n",
    "# plot_and_save_metrics(training_mean_loss, training_accuracy, eval_mean_loss, eval_accuracy, experiment_dir)\n",
    "\n",
    "\n",
    "\n",
    "# Import the necessary modules here\n",
    "from data_utils.S3DISDataLoader import S3DISDataset\n",
    "import provider\n",
    "\n",
    "# Adjusted BASE_DIR and ROOT_DIR setup\n",
    "BASE_DIR = os.getcwd()  # Use the current working directory\n",
    "ROOT_DIR = BASE_DIR\n",
    "sys.path.append(os.path.join(ROOT_DIR, 'models'))\n",
    "\n",
    "# Your class and function definitions remain unchanged...\n",
    "classes = ['ceiling', 'floor', 'wall', 'beam', 'column', 'window', 'door', 'table', 'chair', 'sofa', 'bookcase',\n",
    "           'board', 'clutter']\n",
    "class2label = {cls: i for i, cls in enumerate(classes)}\n",
    "seg_classes = class2label\n",
    "seg_label_to_cat = {}\n",
    "for i, cat in enumerate(seg_classes.keys()):\n",
    "    seg_label_to_cat[i] = cat\n",
    "\n",
    "def inplace_relu(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('ReLU') != -1:\n",
    "        m.inplace=True\n",
    "\n",
    "def parse_args():\n",
    "    \"\"\"Parse command line arguments or set default values if running in a Jupyter notebook/IPython.\"\"\"\n",
    "    if sys.argv[0].endswith('ipykernel_launcher.py') or 'ipykernel' in sys.argv[0]:\n",
    "        # Directly define args for Jupyter/Colab notebooks or IPython\n",
    "        class Args:\n",
    "            model = 'pointnet2_sem_seg_tran'\n",
    "            batch_size = 16\n",
    "            epoch = 32\n",
    "            learning_rate = 0.001\n",
    "            gpu = '0'\n",
    "            optimizer = 'Adam'\n",
    "            log_dir = 'pointnet2_sem_seg_tran'\n",
    "            decay_rate = 1e-4\n",
    "            npoint = 4096\n",
    "            step_size = 10\n",
    "            lr_decay = 0.7\n",
    "            test_area = 2\n",
    "        return Args()\n",
    "    else:\n",
    "        # Original argparse code for command-line execution\n",
    "        parser = argparse.ArgumentParser('Model')\n",
    "        parser.add_argument('--model', type=str, default='pointnet_sem_seg', help='model name [default: pointnet_sem_seg]')\n",
    "        parser.add_argument('--batch_size', type=int, default=16, help='Batch Size during training [default: 16]')\n",
    "        # Add the rest of your arguments here\n",
    "        return parser.parse_args()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    def log_string(str):\n",
    "        logger.info(str)\n",
    "        print(str)\n",
    "\n",
    "    '''HYPER PARAMETER'''\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu\n",
    "\n",
    "    '''CREATE DIR'''\n",
    "    timestr = str(datetime.datetime.now().strftime('%Y-%m-%d_%H-%M'))\n",
    "    experiment_dir = Path('./log/')\n",
    "    experiment_dir.mkdir(exist_ok=True)\n",
    "    experiment_dir = experiment_dir.joinpath('sem_seg')\n",
    "    experiment_dir.mkdir(exist_ok=True)\n",
    "    if args.log_dir is None:\n",
    "        experiment_dir = experiment_dir.joinpath(timestr)\n",
    "    else:\n",
    "        experiment_dir = experiment_dir.joinpath(args.log_dir)\n",
    "    experiment_dir.mkdir(exist_ok=True)\n",
    "    checkpoints_dir = experiment_dir.joinpath('checkpoints/')\n",
    "    checkpoints_dir.mkdir(exist_ok=True)\n",
    "    log_dir = experiment_dir.joinpath('logs/')\n",
    "    log_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    '''LOG'''\n",
    "    args = parse_args()\n",
    "    logger = logging.getLogger(\"Model\")\n",
    "    logger.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    file_handler = logging.FileHandler('%s/%s.txt' % (log_dir, args.model))\n",
    "    file_handler.setLevel(logging.INFO)\n",
    "    file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(file_handler)\n",
    "    log_string('PARAMETER ...')\n",
    "    log_string(args)\n",
    "\n",
    "    root = '/home/puoza/p2/Pointnet_Pointnet2_pytorch/new'\n",
    "    NUM_CLASSES = 13\n",
    "    NUM_POINT = args.npoint\n",
    "    BATCH_SIZE = args.batch_size\n",
    "\n",
    "    print(\"start loading training data ...\")\n",
    "    TRAIN_DATASET = S3DISDataset(split='train', data_root=root, num_point=NUM_POINT, test_area=args.test_area, block_size=1.0, sample_rate=1.0, transform=None)\n",
    "    print(\"start loading test data ...\")\n",
    "    TEST_DATASET = S3DISDataset(split='test', data_root=root, num_point=NUM_POINT, test_area=args.test_area, block_size=1.0, sample_rate=1.0, transform=None)\n",
    "\n",
    "    trainDataLoader = torch.utils.data.DataLoader(TRAIN_DATASET, batch_size=BATCH_SIZE, shuffle=True, num_workers=10,\n",
    "                                                  pin_memory=True, drop_last=True,\n",
    "                                                  worker_init_fn=lambda x: np.random.seed(x + int(time.time())))\n",
    "    testDataLoader = torch.utils.data.DataLoader(TEST_DATASET, batch_size=BATCH_SIZE, shuffle=False, num_workers=10,\n",
    "                                                 pin_memory=True, drop_last=True)\n",
    "    weights = torch.Tensor(TRAIN_DATASET.labelweights).cuda()\n",
    "\n",
    "    log_string(\"The number of training data is: %d\" % len(TRAIN_DATASET))\n",
    "    log_string(\"The number of test data is: %d\" % len(TEST_DATASET))\n",
    "\n",
    "    '''MODEL LOADING'''\n",
    "    MODEL = importlib.import_module(args.model)\n",
    "    shutil.copy('models/%s.py' % args.model, str(experiment_dir))\n",
    "    shutil.copy('models/pointnet2_utils.py', str(experiment_dir))\n",
    "\n",
    "    classifier = MODEL.get_model(NUM_CLASSES).cuda()\n",
    "    criterion = MODEL.get_loss().cuda()\n",
    "    classifier.apply(inplace_relu)\n",
    "\n",
    "    def weights_init(m):\n",
    "        classname = m.__class__.__name__\n",
    "        if classname.find('Conv2d') != -1:\n",
    "            torch.nn.init.xavier_normal_(m.weight.data)\n",
    "            torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "        elif classname.find('Linear') != -1:\n",
    "            torch.nn.init.xavier_normal_(m.weight.data)\n",
    "            torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "    try:\n",
    "        checkpoint = torch.load(str(experiment_dir) + '/checkpoints/best_model.pth')\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        classifier.load_state_dict(checkpoint['model_state_dict'])\n",
    "        log_string('Use pretrain model')\n",
    "    except:\n",
    "        log_string('No existing model, starting training from scratch...')\n",
    "        start_epoch = 0\n",
    "        classifier = classifier.apply(weights_init)\n",
    "\n",
    "    if args.optimizer == 'Adam':\n",
    "        optimizer = torch.optim.Adam(\n",
    "            classifier.parameters(),\n",
    "            lr=args.learning_rate,\n",
    "            betas=(0.9, 0.999),\n",
    "            eps=1e-08,\n",
    "            weight_decay=args.decay_rate\n",
    "        )\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD(classifier.parameters(), lr=args.learning_rate, momentum=0.9)\n",
    "\n",
    "    def bn_momentum_adjust(m, momentum):\n",
    "        if isinstance(m, torch.nn.BatchNorm2d) or isinstance(m, torch.nn.BatchNorm1d):\n",
    "            m.momentum = momentum\n",
    "\n",
    "    LEARNING_RATE_CLIP = 1e-5\n",
    "    MOMENTUM_ORIGINAL = 0.1\n",
    "    MOMENTUM_DECCAY = 0.5\n",
    "    MOMENTUM_DECCAY_STEP = args.step_size\n",
    "\n",
    "    global_epoch = 0\n",
    "    best_iou = 0\n",
    "\n",
    "    for epoch in range(start_epoch, args.epoch):\n",
    "        '''Train on chopped scenes'''\n",
    "        log_string('**** Epoch %d (%d/%s) ****' % (global_epoch + 1, epoch + 1, args.epoch))\n",
    "        lr = max(args.learning_rate * (args.lr_decay ** (epoch // args.step_size)), LEARNING_RATE_CLIP)\n",
    "        log_string('Learning rate:%f' % lr)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        momentum = MOMENTUM_ORIGINAL * (MOMENTUM_DECCAY ** (epoch // MOMENTUM_DECCAY_STEP))\n",
    "        if momentum < 0.01:\n",
    "            momentum = 0.01\n",
    "        print('BN momentum updated to: %f' % momentum)\n",
    "        classifier = classifier.apply(lambda x: bn_momentum_adjust(x, momentum))\n",
    "        num_batches = len(trainDataLoader)\n",
    "        total_correct = 0\n",
    "        total_seen = 0\n",
    "        loss_sum = 0\n",
    "        classifier = classifier.train()\n",
    "\n",
    "        for i, (points, target) in tqdm(enumerate(trainDataLoader), total=len(trainDataLoader), smoothing=0.9):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            points = points.data.numpy()\n",
    "            points[:, :, :3] = provider.rotate_point_cloud_z(points[:, :, :3])\n",
    "            points = torch.Tensor(points)\n",
    "            points, target = points.float().cuda(), target.long().cuda()\n",
    "            points = points.transpose(2, 1)\n",
    "\n",
    "            seg_pred, trans_feat = classifier(points)\n",
    "            seg_pred = seg_pred.contiguous().view(-1, NUM_CLASSES)\n",
    "\n",
    "            batch_label = target.view(-1, 1)[:, 0].cpu().data.numpy()\n",
    "            target = target.view(-1, 1)[:, 0]\n",
    "            loss = criterion(seg_pred, target, trans_feat, weights)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            pred_choice = seg_pred.cpu().data.max(1)[1].numpy()\n",
    "            correct = np.sum(pred_choice == batch_label)\n",
    "            total_correct += correct\n",
    "            total_seen += (BATCH_SIZE * NUM_POINT)\n",
    "            loss_sum += loss\n",
    "        log_string('Training mean loss: %f' % (loss_sum / num_batches))\n",
    "        log_string('Training accuracy: %f' % (total_correct / float(total_seen)))\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            logger.info('Save model...')\n",
    "            savepath = str(checkpoints_dir) + '/model.pth'\n",
    "            log_string('Saving at %s' % savepath)\n",
    "            state = {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': classifier.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }\n",
    "            torch.save(state, savepath)\n",
    "            log_string('Saving model....')\n",
    "\n",
    "        '''Evaluate on chopped scenes'''\n",
    "        with torch.no_grad():\n",
    "            num_batches = len(testDataLoader)\n",
    "            total_correct = 0\n",
    "            total_seen = 0\n",
    "            loss_sum = 0\n",
    "            labelweights = np.zeros(NUM_CLASSES)\n",
    "            total_seen_class = [0 for _ in range(NUM_CLASSES)]\n",
    "            total_correct_class = [0 for _ in range(NUM_CLASSES)]\n",
    "            total_iou_deno_class = [0 for _ in range(NUM_CLASSES)]\n",
    "            classifier = classifier.eval()\n",
    "\n",
    "            log_string('---- EPOCH %03d EVALUATION ----' % (global_epoch + 1))\n",
    "            for i, (points, target) in tqdm(enumerate(testDataLoader), total=len(testDataLoader), smoothing=0.9):\n",
    "                points = points.data.numpy()\n",
    "                points = torch.Tensor(points)\n",
    "                points, target = points.float().cuda(), target.long().cuda()\n",
    "                points = points.transpose(2, 1)\n",
    "\n",
    "                seg_pred, trans_feat = classifier(points)\n",
    "                pred_val = seg_pred.contiguous().cpu().data.numpy()\n",
    "                seg_pred = seg_pred.contiguous().view(-1, NUM_CLASSES)\n",
    "\n",
    "                batch_label = target.cpu().data.numpy()\n",
    "                target = target.view(-1, 1)[:, 0]\n",
    "                loss = criterion(seg_pred, target, trans_feat, weights)\n",
    "                loss_sum += loss\n",
    "                pred_val = np.argmax(pred_val, 2)\n",
    "                correct = np.sum((pred_val == batch_label))\n",
    "                total_correct += correct\n",
    "                total_seen += (BATCH_SIZE * NUM_POINT)\n",
    "                tmp, _ = np.histogram(batch_label, range(NUM_CLASSES + 1))\n",
    "                labelweights += tmp\n",
    "\n",
    "                for l in range(NUM_CLASSES):\n",
    "                    total_seen_class[l] += np.sum((batch_label == l))\n",
    "                    total_correct_class[l] += np.sum((pred_val == l) & (batch_label == l))\n",
    "                    total_iou_deno_class[l] += np.sum(((pred_val == l) | (batch_label == l)))\n",
    "\n",
    "            labelweights = labelweights.astype(float) / np.sum(labelweights.astype(float))\n",
    "            mIoU = np.mean(np.array(total_correct_class) / (np.array(total_iou_deno_class, dtype=float) + 1e-6))\n",
    "            log_string('eval mean loss: %f' % (loss_sum / float(num_batches)))\n",
    "            log_string('eval point avg class IoU: %f' % (mIoU))\n",
    "            log_string('eval point accuracy: %f' % (total_correct / float(total_seen)))\n",
    "            log_string('eval point avg class acc: %f' % (\n",
    "                np.mean(np.array(total_correct_class) / (np.array(total_seen_class, dtype=float) + 1e-6))))\n",
    "\n",
    "            iou_per_class_str = '------- IoU --------\\n'\n",
    "            for l in range(NUM_CLASSES):\n",
    "                iou_per_class_str += 'class %s weight: %.3f, IoU: %.3f \\n' % (\n",
    "                    seg_label_to_cat[l] + ' ' * (14 - len(seg_label_to_cat[l])), labelweights[l - 1],\n",
    "                    total_correct_class[l] / float(total_iou_deno_class[l]))\n",
    "\n",
    "            log_string(iou_per_class_str)\n",
    "            log_string('Eval mean loss: %f' % (loss_sum / num_batches))\n",
    "            log_string('Eval accuracy: %f' % (total_correct / float(total_seen)))\n",
    "\n",
    "            if mIoU >= best_iou:\n",
    "                best_iou = mIoU\n",
    "                logger.info('Save model...')\n",
    "                savepath = str(checkpoints_dir) + '/best_model.pth'\n",
    "                log_string('Saving at %s' % savepath)\n",
    "                state = {\n",
    "                    'epoch': epoch,\n",
    "                    'class_avg_iou': mIoU,\n",
    "                    'model_state_dict': classifier.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                }\n",
    "                torch.save(state, savepath)\n",
    "                log_string('Saving model....')\n",
    "            log_string('Best mIoU: %f' % best_iou)\n",
    "        global_epoch += 1\n",
    "        #plot_and_save_metrics(training_mean_loss, training_accuracy, eval_mean_loss, eval_accuracy, experiment_dir)\n",
    "        #save_metrics(training_mean_loss, training_accuracy, eval_mean_loss, eval_accuracy, experiment_dir)\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    args = parse_args()\n",
    "    main(args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76606168-ca38-4163-b5f5-2f8bf776f891",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e484bbe-838c-4372-b2dc-8a0c05cf7106",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca3337d-0ee5-411e-a74e-885008d1cfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import logging\n",
    "import sys\n",
    "import importlib\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Import the necessary modules here\n",
    "from data_utils.S3DISDataLoader import ScannetDatasetWholeScene\n",
    "from data_utils.indoor3d_util import g_label2color\n",
    "import provider\n",
    "\n",
    "BASE_DIR = os.getcwd() \n",
    "ROOT_DIR = BASE_DIR\n",
    "sys.path.append(os.path.join(ROOT_DIR, 'models'))\n",
    "\n",
    "classes = ['ceiling', 'floor', 'wall', 'beam', 'column', 'window', 'door', 'table', 'chair', 'sofa', 'bookcase',\n",
    "           'board', 'clutter']\n",
    "class2label = {cls: i for i, cls in enumerate(classes)}\n",
    "seg_classes = class2label\n",
    "seg_label_to_cat = {}\n",
    "for i, cat in enumerate(seg_classes.keys()):\n",
    "    seg_label_to_cat[i] = cat\n",
    "\n",
    "\n",
    "def parse_args(argv=None):\n",
    "    '''PARAMETERS'''\n",
    "    parser = argparse.ArgumentParser('Model')\n",
    "    parser.add_argument('--batch_size', type=int, default=32, help='batch size in testing [default: 32]')\n",
    "    parser.add_argument('--gpu', type=str, default='0', help='specify gpu device')\n",
    "    parser.add_argument('--num_point', type=int, default=4096, help='point number [default: 4096]')\n",
    "    parser.add_argument('--log_dir', type=str, default='pointnet2_sem_seg_msg', help='experiment root')\n",
    "    parser.add_argument('--visual', action='store_true', default=True, help='visualize result [default: False]')\n",
    "    parser.add_argument('--test_area', type=int, default=5, help='area for testing, option: 1-6 [default: 5]')\n",
    "    parser.add_argument('--num_votes', type=int, default=3, help='aggregate segmentation scores with voting [default: 5]')\n",
    "    if argv is not None:\n",
    "        return parser.parse_args(argv)\n",
    "    else:\n",
    "        return parser.parse_args()\n",
    "\n",
    "\n",
    "def add_vote(vote_label_pool, point_idx, pred_label, weight):\n",
    "    B = pred_label.shape[0]\n",
    "    N = pred_label.shape[1]\n",
    "    for b in range(B):\n",
    "        for n in range(N):\n",
    "            if weight[b, n] != 0 and not np.isinf(weight[b, n]):\n",
    "                vote_label_pool[int(point_idx[b, n]), int(pred_label[b, n])] += 1\n",
    "    return vote_label_pool\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    def log_string(str):\n",
    "        logger.info(str)\n",
    "        print(str)\n",
    "\n",
    "    '''HYPER PARAMETER'''\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu\n",
    "    experiment_dir = 'log/sem_seg/' + args.log_dir\n",
    "    visual_dir = experiment_dir + '/visual/'\n",
    "    visual_dir = Path(visual_dir)\n",
    "    visual_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    '''LOG'''\n",
    "    logger = logging.getLogger(\"Model\")\n",
    "    logger.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    file_handler = logging.FileHandler('%s/eval.txt' % experiment_dir)\n",
    "    file_handler.setLevel(logging.INFO)\n",
    "    file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(file_handler)\n",
    "    log_string('PARAMETER ...')\n",
    "    log_string(args)\n",
    "\n",
    "    NUM_CLASSES = 13\n",
    "    BATCH_SIZE = args.batch_size\n",
    "    NUM_POINT = args.num_point\n",
    "\n",
    "    root = '/home/puoza/p2/Pointnet_Pointnet2_pytorch/stanford_indoor3d/'\n",
    "\n",
    "    TEST_DATASET_WHOLE_SCENE = ScannetDatasetWholeScene(root, split='test', test_area=args.test_area, block_points=NUM_POINT)\n",
    "    log_string(\"The number of test data is: %d\" % len(TEST_DATASET_WHOLE_SCENE))\n",
    "\n",
    "    transformer_input_dim = 1024  # Example value, adjust based on your model's architecture\n",
    "    transformer_num_heads = 8    # Example value, typically a power of 2\n",
    "    transformer_hidden_dim = 1024\n",
    "\n",
    "    '''MODEL LOADING'''\n",
    "    model_name = os.listdir(experiment_dir + '/logs')[0].split('.')[0]\n",
    "    MODEL = importlib.import_module(model_name)\n",
    "    classifier = MODEL.get_model(NUM_CLASSES, transformer_input_dim, transformer_num_heads, \n",
    "    transformer_hidden_dim).cuda()\n",
    "    checkpoint = torch.load(str(experiment_dir) + '/checkpoints/best_model.pth')\n",
    "    classifier.load_state_dict(checkpoint['model_state_dict'])\n",
    "    classifier = classifier.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        scene_id = TEST_DATASET_WHOLE_SCENE.file_list\n",
    "        scene_id = [x[:-4] for x in scene_id]\n",
    "        num_batches = len(TEST_DATASET_WHOLE_SCENE)\n",
    "\n",
    "        total_seen_class = [0 for _ in range(NUM_CLASSES)]\n",
    "        total_correct_class = [0 for _ in range(NUM_CLASSES)]\n",
    "        total_iou_deno_class = [0 for _ in range(NUM_CLASSES)]\n",
    "\n",
    "        log_string('---- EVALUATION WHOLE SCENE----')\n",
    "\n",
    "        for batch_idx in range(num_batches):\n",
    "            print(\"Inference [%d/%d] %s ...\" % (batch_idx + 1, num_batches, scene_id[batch_idx]))\n",
    "            total_seen_class_tmp = [0 for _ in range(NUM_CLASSES)]\n",
    "            total_correct_class_tmp = [0 for _ in range(NUM_CLASSES)]\n",
    "            total_iou_deno_class_tmp = [0 for _ in range(NUM_CLASSES)]\n",
    "            if args.visual:\n",
    "                fout = open(os.path.join(visual_dir, scene_id[batch_idx] + '_pred.obj'), 'w')\n",
    "                fout_gt = open(os.path.join(visual_dir, scene_id[batch_idx] + '_gt.obj'), 'w')\n",
    "\n",
    "            whole_scene_data = TEST_DATASET_WHOLE_SCENE.scene_points_list[batch_idx]\n",
    "            whole_scene_label = TEST_DATASET_WHOLE_SCENE.semantic_labels_list[batch_idx]\n",
    "            vote_label_pool = np.zeros((whole_scene_label.shape[0], NUM_CLASSES))\n",
    "            for _ in tqdm(range(args.num_votes), total=args.num_votes):\n",
    "                scene_data, scene_label, scene_smpw, scene_point_index = TEST_DATASET_WHOLE_SCENE[batch_idx]\n",
    "                num_blocks = scene_data.shape[0]\n",
    "                s_batch_num = (num_blocks + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "                batch_data = np.zeros((BATCH_SIZE, NUM_POINT, 9))\n",
    "\n",
    "                batch_label = np.zeros((BATCH_SIZE, NUM_POINT))\n",
    "                batch_point_index = np.zeros((BATCH_SIZE, NUM_POINT))\n",
    "                batch_smpw = np.zeros((BATCH_SIZE, NUM_POINT))\n",
    "\n",
    "                for sbatch in range(s_batch_num):\n",
    "                    start_idx = sbatch * BATCH_SIZE\n",
    "                    end_idx = min((sbatch + 1) * BATCH_SIZE, num_blocks)\n",
    "                    real_batch_size = end_idx - start_idx\n",
    "                    batch_data[0:real_batch_size, ...] = scene_data[start_idx:end_idx, ...]\n",
    "                    batch_label[0:real_batch_size, ...] = scene_label[start_idx:end_idx, ...]\n",
    "                    batch_point_index[0:real_batch_size, ...] = scene_point_index[start_idx:end_idx, ...]\n",
    "                    batch_smpw[0:real_batch_size, ...] = scene_smpw[start_idx:end_idx, ...]\n",
    "                    batch_data[:, :, 3:6] /= 1.0\n",
    "\n",
    "                    torch_data = torch.Tensor(batch_data)\n",
    "                    torch_data = torch_data.float().cuda()\n",
    "                    torch_data = torch_data.transpose(2, 1)\n",
    "                    seg_pred, _ = classifier(torch_data)\n",
    "                    batch_pred_label = seg_pred.contiguous().cpu().data.max(2)[1].numpy()\n",
    "\n",
    "                    vote_label_pool = add_vote(vote_label_pool, batch_point_index[0:real_batch_size, ...],\n",
    "                                               batch_pred_label[0:real_batch_size, ...],\n",
    "                                               batch_smpw[0:real_batch_size, ...])\n",
    "\n",
    "            pred_label = np.argmax(vote_label_pool, 1)\n",
    "\n",
    "            for l in range(NUM_CLASSES):\n",
    "                total_seen_class_tmp[l] += np.sum((whole_scene_label == l))\n",
    "                total_correct_class_tmp[l] += np.sum((pred_label == l) & (whole_scene_label == l))\n",
    "                total_iou_deno_class_tmp[l] += np.sum(((pred_label == l) | (whole_scene_label == l)))\n",
    "                total_seen_class[l] += total_seen_class_tmp[l]\n",
    "                total_correct_class[l] += total_correct_class_tmp[l]\n",
    "                total_iou_deno_class[l] += total_iou_deno_class_tmp[l]\n",
    "\n",
    "            iou_map = np.array(total_correct_class_tmp) / (np.array(total_iou_deno_class_tmp, dtype=float) + 1e-6)\n",
    "            print(iou_map)\n",
    "            arr = np.array(total_seen_class_tmp)\n",
    "            tmp_iou = np.mean(iou_map[arr != 0])\n",
    "            log_string('Mean IoU of %s: %.4f' % (scene_id[batch_idx], tmp_iou))\n",
    "            print('----------------------------')\n",
    "\n",
    "            filename = os.path.join(visual_dir, scene_id[batch_idx] + '.txt')\n",
    "            with open(filename, 'w') as pl_save:\n",
    "                for i in pred_label:\n",
    "                    pl_save.write(str(int(i)) + '\\n')\n",
    "                pl_save.close()\n",
    "            for i in range(whole_scene_label.shape[0]):\n",
    "                color = g_label2color[pred_label[i]]\n",
    "                color_gt = g_label2color[whole_scene_label[i]]\n",
    "                if args.visual:\n",
    "                    fout.write('v %f %f %f %d %d %d\\n' % (\n",
    "                        whole_scene_data[i, 0], whole_scene_data[i, 1], whole_scene_data[i, 2], color[0], color[1],\n",
    "                        color[2]))\n",
    "                    fout_gt.write(\n",
    "                        'v %f %f %f %d %d %d\\n' % (\n",
    "                            whole_scene_data[i, 0], whole_scene_data[i, 1], whole_scene_data[i, 2], color_gt[0],\n",
    "                            color_gt[1], color_gt[2]))\n",
    "            if args.visual:\n",
    "                fout.close()\n",
    "                fout_gt.close()\n",
    "\n",
    "        IoU = np.array(total_correct_class) / (np.array(total_iou_deno_class, dtype=float) + 1e-6)\n",
    "        iou_per_class_str = '------- IoU --------\\n'\n",
    "        for l in range(NUM_CLASSES):\n",
    "            iou_per_class_str += 'class %s, IoU: %.3f \\n' % (\n",
    "                seg_label_to_cat[l] + ' ' * (14 - len(seg_label_to_cat[l])),\n",
    "                total_correct_class[l] / float(total_iou_deno_class[l]))\n",
    "        log_string(iou_per_class_str)\n",
    "        log_string('eval point avg class IoU: %f' % np.mean(IoU))\n",
    "        log_string('eval whole scene point avg class acc: %f' % (\n",
    "            np.mean(np.array(total_correct_class) / (np.array(total_seen_class, dtype=float) + 1e-6))))\n",
    "        log_string('eval whole scene point accuracy: %f' % (\n",
    "                np.sum(total_correct_class) / float(np.sum(total_seen_class) + 1e-6)))\n",
    "\n",
    "        print(\"Done!\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import sys\n",
    "    if 'ipykernel' in sys.modules:\n",
    "        # Running in a Jupyter Notebook, skip argument parsing\n",
    "        args = parse_args([])\n",
    "    else:\n",
    "        # Running as the main program, parse command-line arguments\n",
    "        args = parse_args()\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fcf1d7-2219-454e-9d87-78372737ffa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3c8046-d442-4a01-af36-5b3d59957e7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu-2.1.0-cuda-12.1",
   "language": "python",
   "name": "pytorch-gpu-2.1.0-cuda-12.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
