{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26151e1-65f0-4ac4-abe5-de12a3d2009f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch\n",
    "!python train_semseg.py --model pointnet2_sem_seg_tran_msg --test_area 2 --log_dir pointnet2_sem_seg_tran_msg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "401d1ea7-3fb7-439e-beea-066fd4b70107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARAMETER ...\n",
      "Namespace(batch_size=32, gpu='0', num_point=4096, log_dir='pointnet2_sem_seg', visual=True, test_area=1, num_votes=3)\n",
      "The number of test data is: 30\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/puoza/p2/Pointnet_Pointnet2_pytorch/test_semseg.py\", line 205, in <module>\n",
      "    main(args)\n",
      "  File \"/home/puoza/p2/Pointnet_Pointnet2_pytorch/test_semseg.py\", line 91, in main\n",
      "    MODEL = importlib.import_module(model_name)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/packages/apps/jupyter/2023-10-09/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<frozen importlib._bootstrap>\", line 1201, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1114, in _sanity_check\n",
      "ValueError: Empty module name\n"
     ]
    }
   ],
   "source": [
    "!python test_semseg.py --log_dir pointnet2_sem_seg --test_area 1 --visual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cb2fee-f8d8-4294-9d74-ac2289479aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import datetime\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import importlib\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def save_metrics(training_mean_loss, training_accuracy, eval_mean_loss, eval_accuracy, file_path):\n",
    "    with open(file_path, 'a') as f:\n",
    "        f.write(f\"{training_mean_loss[-1]},{training_accuracy[-1]},{eval_mean_loss[-1]},{eval_accuracy[-1]}\\n\")\n",
    "\n",
    "def plot_and_save_metrics(training_mean_loss, training_accuracy, eval_mean_loss, eval_accuracy, experiment_dir):\n",
    "    # Move tensors to CPU\n",
    "    training_mean_loss_cpu = [val.item() for val in training_mean_loss]\n",
    "    training_accuracy_cpu = [val.item() for val in training_accuracy]\n",
    "    eval_mean_loss_cpu = [val.item() for val in eval_mean_loss]\n",
    "    eval_accuracy_cpu = [val.item() for val in eval_accuracy]\n",
    "\n",
    "    # Plot training mean loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(training_mean_loss_cpu, label='Training Mean Loss', color='blue')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Loss')\n",
    "    plt.title('Training Mean Loss per Epoch')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(str(experiment_dir) + '/training_mean_loss.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Plot training accuracy\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(training_accuracy_cpu, label='Training Accuracy', color='green')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training Accuracy per Epoch')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(str(experiment_dir) + '/training_accuracy.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Plot evaluation mean loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(eval_mean_loss_cpu, label='Evaluation Mean Loss', color='red')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Loss')\n",
    "    plt.title('Evaluation Mean Loss per Epoch')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(str(experiment_dir) + '/eval_mean_loss.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Plot evaluation accuracy\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(eval_accuracy_cpu, label='Evaluation Accuracy', color='purple')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Evaluation Accuracy per Epoch')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(str(experiment_dir) + '/eval_accuracy.png')\n",
    "    plt.close()\n",
    "# Usage:\n",
    "# plot_and_save_metrics(training_mean_loss, training_accuracy, eval_mean_loss, eval_accuracy, experiment_dir)\n",
    "\n",
    "\n",
    "\n",
    "# Import the necessary modules here\n",
    "from data_utils.S3DISDataLoader import S3DISDataset\n",
    "import provider\n",
    "\n",
    "# Adjusted BASE_DIR and ROOT_DIR setup\n",
    "BASE_DIR = os.getcwd()  # Use the current working directory\n",
    "ROOT_DIR = BASE_DIR\n",
    "sys.path.append(os.path.join(ROOT_DIR, 'models'))\n",
    "\n",
    "# Your class and function definitions remain unchanged...\n",
    "classes = ['ceiling', 'floor', 'wall', 'beam', 'column', 'window', 'door', 'table', 'chair', 'sofa', 'bookcase',\n",
    "           'board', 'clutter']\n",
    "class2label = {cls: i for i, cls in enumerate(classes)}\n",
    "seg_classes = class2label\n",
    "seg_label_to_cat = {}\n",
    "for i, cat in enumerate(seg_classes.keys()):\n",
    "    seg_label_to_cat[i] = cat\n",
    "\n",
    "def inplace_relu(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('ReLU') != -1:\n",
    "        m.inplace=True\n",
    "\n",
    "def parse_args():\n",
    "    \"\"\"Parse command line arguments or set default values if running in a Jupyter notebook/IPython.\"\"\"\n",
    "    if sys.argv[0].endswith('ipykernel_launcher.py') or 'ipykernel' in sys.argv[0]:\n",
    "        # Directly define args for Jupyter/Colab notebooks or IPython\n",
    "        class Args:\n",
    "            model = 'pointnet2_sem_seg_tran_msg'\n",
    "            batch_size = 16\n",
    "            epoch = 32\n",
    "            learning_rate = 0.001\n",
    "            gpu = '0'\n",
    "            optimizer = 'Adam'\n",
    "            log_dir = 'pointnet2_sem_seg_tran_msg'\n",
    "            decay_rate = 1e-4\n",
    "            npoint = 4096\n",
    "            step_size = 10\n",
    "            lr_decay = 0.7\n",
    "            test_area = 2\n",
    "        return Args()\n",
    "    else:\n",
    "        # Original argparse code for command-line execution\n",
    "        parser = argparse.ArgumentParser('Model')\n",
    "        parser.add_argument('--model', type=str, default='pointnet_sem_seg', help='model name [default: pointnet_sem_seg]')\n",
    "        parser.add_argument('--batch_size', type=int, default=16, help='Batch Size during training [default: 16]')\n",
    "        # Add the rest of your arguments here\n",
    "        return parser.parse_args()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    def log_string(str):\n",
    "        logger.info(str)\n",
    "        print(str)\n",
    "\n",
    "    '''HYPER PARAMETER'''\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu\n",
    "\n",
    "    '''CREATE DIR'''\n",
    "    timestr = str(datetime.datetime.now().strftime('%Y-%m-%d_%H-%M'))\n",
    "    experiment_dir = Path('./log/')\n",
    "    experiment_dir.mkdir(exist_ok=True)\n",
    "    experiment_dir = experiment_dir.joinpath('sem_seg')\n",
    "    experiment_dir.mkdir(exist_ok=True)\n",
    "    if args.log_dir is None:\n",
    "        experiment_dir = experiment_dir.joinpath(timestr)\n",
    "    else:\n",
    "        experiment_dir = experiment_dir.joinpath(args.log_dir)\n",
    "    experiment_dir.mkdir(exist_ok=True)\n",
    "    checkpoints_dir = experiment_dir.joinpath('checkpoints/')\n",
    "    checkpoints_dir.mkdir(exist_ok=True)\n",
    "    log_dir = experiment_dir.joinpath('logs/')\n",
    "    log_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    '''LOG'''\n",
    "    args = parse_args()\n",
    "    logger = logging.getLogger(\"Model\")\n",
    "    logger.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    file_handler = logging.FileHandler('%s/%s.txt' % (log_dir, args.model))\n",
    "    file_handler.setLevel(logging.INFO)\n",
    "    file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(file_handler)\n",
    "    log_string('PARAMETER ...')\n",
    "    log_string(args)\n",
    "\n",
    "    root = '/home/puoza/p2/Pointnet_Pointnet2_pytorch/new'\n",
    "    NUM_CLASSES = 13\n",
    "    NUM_POINT = args.npoint\n",
    "    BATCH_SIZE = args.batch_size\n",
    "\n",
    "    print(\"start loading training data ...\")\n",
    "    TRAIN_DATASET = S3DISDataset(split='train', data_root=root, num_point=NUM_POINT, test_area=args.test_area, block_size=1.0, sample_rate=1.0, transform=None)\n",
    "    print(\"start loading test data ...\")\n",
    "    TEST_DATASET = S3DISDataset(split='test', data_root=root, num_point=NUM_POINT, test_area=args.test_area, block_size=1.0, sample_rate=1.0, transform=None)\n",
    "\n",
    "    trainDataLoader = torch.utils.data.DataLoader(TRAIN_DATASET, batch_size=BATCH_SIZE, shuffle=True, num_workers=10,\n",
    "                                                  pin_memory=True, drop_last=True,\n",
    "                                                  worker_init_fn=lambda x: np.random.seed(x + int(time.time())))\n",
    "    testDataLoader = torch.utils.data.DataLoader(TEST_DATASET, batch_size=BATCH_SIZE, shuffle=False, num_workers=10,\n",
    "                                                 pin_memory=True, drop_last=True)\n",
    "    weights = torch.Tensor(TRAIN_DATASET.labelweights).cuda()\n",
    "\n",
    "    log_string(\"The number of training data is: %d\" % len(TRAIN_DATASET))\n",
    "    log_string(\"The number of test data is: %d\" % len(TEST_DATASET))\n",
    "\n",
    "    '''MODEL LOADING'''\n",
    "    MODEL = importlib.import_module(args.model)\n",
    "    shutil.copy('models/%s.py' % args.model, str(experiment_dir))\n",
    "    shutil.copy('models/pointnet2_utils.py', str(experiment_dir))\n",
    "\n",
    "    classifier = MODEL.get_model(NUM_CLASSES).cuda()\n",
    "    criterion = MODEL.get_loss().cuda()\n",
    "    classifier.apply(inplace_relu)\n",
    "\n",
    "    def weights_init(m):\n",
    "        classname = m.__class__.__name__\n",
    "        if classname.find('Conv2d') != -1:\n",
    "            torch.nn.init.xavier_normal_(m.weight.data)\n",
    "            torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "        elif classname.find('Linear') != -1:\n",
    "            torch.nn.init.xavier_normal_(m.weight.data)\n",
    "            torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "    try:\n",
    "        checkpoint = torch.load(str(experiment_dir) + '/checkpoints/best_model.pth')\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        classifier.load_state_dict(checkpoint['model_state_dict'])\n",
    "        log_string('Use pretrain model')\n",
    "    except:\n",
    "        log_string('No existing model, starting training from scratch...')\n",
    "        start_epoch = 0\n",
    "        classifier = classifier.apply(weights_init)\n",
    "\n",
    "    if args.optimizer == 'Adam':\n",
    "        optimizer = torch.optim.Adam(\n",
    "            classifier.parameters(),\n",
    "            lr=args.learning_rate,\n",
    "            betas=(0.9, 0.999),\n",
    "            eps=1e-08,\n",
    "            weight_decay=args.decay_rate\n",
    "        )\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD(classifier.parameters(), lr=args.learning_rate, momentum=0.9)\n",
    "\n",
    "    def bn_momentum_adjust(m, momentum):\n",
    "        if isinstance(m, torch.nn.BatchNorm2d) or isinstance(m, torch.nn.BatchNorm1d):\n",
    "            m.momentum = momentum\n",
    "\n",
    "    LEARNING_RATE_CLIP = 1e-5\n",
    "    MOMENTUM_ORIGINAL = 0.1\n",
    "    MOMENTUM_DECCAY = 0.5\n",
    "    MOMENTUM_DECCAY_STEP = args.step_size\n",
    "\n",
    "    global_epoch = 0\n",
    "    best_iou = 0\n",
    "\n",
    "    for epoch in range(start_epoch, args.epoch):\n",
    "        '''Train on chopped scenes'''\n",
    "        log_string('**** Epoch %d (%d/%s) ****' % (global_epoch + 1, epoch + 1, args.epoch))\n",
    "        lr = max(args.learning_rate * (args.lr_decay ** (epoch // args.step_size)), LEARNING_RATE_CLIP)\n",
    "        log_string('Learning rate:%f' % lr)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        momentum = MOMENTUM_ORIGINAL * (MOMENTUM_DECCAY ** (epoch // MOMENTUM_DECCAY_STEP))\n",
    "        if momentum < 0.01:\n",
    "            momentum = 0.01\n",
    "        print('BN momentum updated to: %f' % momentum)\n",
    "        classifier = classifier.apply(lambda x: bn_momentum_adjust(x, momentum))\n",
    "        num_batches = len(trainDataLoader)\n",
    "        total_correct = 0\n",
    "        total_seen = 0\n",
    "        loss_sum = 0\n",
    "        classifier = classifier.train()\n",
    "\n",
    "        for i, (points, target) in tqdm(enumerate(trainDataLoader), total=len(trainDataLoader), smoothing=0.9):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            points = points.data.numpy()\n",
    "            points[:, :, :3] = provider.rotate_point_cloud_z(points[:, :, :3])\n",
    "            points = torch.Tensor(points)\n",
    "            points, target = points.float().cuda(), target.long().cuda()\n",
    "            points = points.transpose(2, 1)\n",
    "\n",
    "            seg_pred, trans_feat = classifier(points)\n",
    "            seg_pred = seg_pred.contiguous().view(-1, NUM_CLASSES)\n",
    "\n",
    "            batch_label = target.view(-1, 1)[:, 0].cpu().data.numpy()\n",
    "            target = target.view(-1, 1)[:, 0]\n",
    "            loss = criterion(seg_pred, target, trans_feat, weights)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            pred_choice = seg_pred.cpu().data.max(1)[1].numpy()\n",
    "            correct = np.sum(pred_choice == batch_label)\n",
    "            total_correct += correct\n",
    "            total_seen += (BATCH_SIZE * NUM_POINT)\n",
    "            loss_sum += loss\n",
    "        log_string('Training mean loss: %f' % (loss_sum / num_batches))\n",
    "        log_string('Training accuracy: %f' % (total_correct / float(total_seen)))\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            logger.info('Save model...')\n",
    "            savepath = str(checkpoints_dir) + '/model.pth'\n",
    "            log_string('Saving at %s' % savepath)\n",
    "            state = {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': classifier.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }\n",
    "            torch.save(state, savepath)\n",
    "            log_string('Saving model....')\n",
    "\n",
    "        '''Evaluate on chopped scenes'''\n",
    "        with torch.no_grad():\n",
    "            num_batches = len(testDataLoader)\n",
    "            total_correct = 0\n",
    "            total_seen = 0\n",
    "            loss_sum = 0\n",
    "            labelweights = np.zeros(NUM_CLASSES)\n",
    "            total_seen_class = [0 for _ in range(NUM_CLASSES)]\n",
    "            total_correct_class = [0 for _ in range(NUM_CLASSES)]\n",
    "            total_iou_deno_class = [0 for _ in range(NUM_CLASSES)]\n",
    "            classifier = classifier.eval()\n",
    "\n",
    "            log_string('---- EPOCH %03d EVALUATION ----' % (global_epoch + 1))\n",
    "            for i, (points, target) in tqdm(enumerate(testDataLoader), total=len(testDataLoader), smoothing=0.9):\n",
    "                points = points.data.numpy()\n",
    "                points = torch.Tensor(points)\n",
    "                points, target = points.float().cuda(), target.long().cuda()\n",
    "                points = points.transpose(2, 1)\n",
    "\n",
    "                seg_pred, trans_feat = classifier(points)\n",
    "                pred_val = seg_pred.contiguous().cpu().data.numpy()\n",
    "                seg_pred = seg_pred.contiguous().view(-1, NUM_CLASSES)\n",
    "\n",
    "                batch_label = target.cpu().data.numpy()\n",
    "                target = target.view(-1, 1)[:, 0]\n",
    "                loss = criterion(seg_pred, target, trans_feat, weights)\n",
    "                loss_sum += loss\n",
    "                pred_val = np.argmax(pred_val, 2)\n",
    "                correct = np.sum((pred_val == batch_label))\n",
    "                total_correct += correct\n",
    "                total_seen += (BATCH_SIZE * NUM_POINT)\n",
    "                tmp, _ = np.histogram(batch_label, range(NUM_CLASSES + 1))\n",
    "                labelweights += tmp\n",
    "\n",
    "                for l in range(NUM_CLASSES):\n",
    "                    total_seen_class[l] += np.sum((batch_label == l))\n",
    "                    total_correct_class[l] += np.sum((pred_val == l) & (batch_label == l))\n",
    "                    total_iou_deno_class[l] += np.sum(((pred_val == l) | (batch_label == l)))\n",
    "\n",
    "            labelweights = labelweights.astype(float) / np.sum(labelweights.astype(float))\n",
    "            mIoU = np.mean(np.array(total_correct_class) / (np.array(total_iou_deno_class, dtype=float) + 1e-6))\n",
    "            log_string('eval mean loss: %f' % (loss_sum / float(num_batches)))\n",
    "            log_string('eval point avg class IoU: %f' % (mIoU))\n",
    "            log_string('eval point accuracy: %f' % (total_correct / float(total_seen)))\n",
    "            log_string('eval point avg class acc: %f' % (\n",
    "                np.mean(np.array(total_correct_class) / (np.array(total_seen_class, dtype=float) + 1e-6))))\n",
    "\n",
    "            iou_per_class_str = '------- IoU --------\\n'\n",
    "            for l in range(NUM_CLASSES):\n",
    "                iou_per_class_str += 'class %s weight: %.3f, IoU: %.3f \\n' % (\n",
    "                    seg_label_to_cat[l] + ' ' * (14 - len(seg_label_to_cat[l])), labelweights[l - 1],\n",
    "                    total_correct_class[l] / float(total_iou_deno_class[l]))\n",
    "\n",
    "            log_string(iou_per_class_str)\n",
    "            log_string('Eval mean loss: %f' % (loss_sum / num_batches))\n",
    "            log_string('Eval accuracy: %f' % (total_correct / float(total_seen)))\n",
    "\n",
    "            if mIoU >= best_iou:\n",
    "                best_iou = mIoU\n",
    "                logger.info('Save model...')\n",
    "                savepath = str(checkpoints_dir) + '/best_model.pth'\n",
    "                log_string('Saving at %s' % savepath)\n",
    "                state = {\n",
    "                    'epoch': epoch,\n",
    "                    'class_avg_iou': mIoU,\n",
    "                    'model_state_dict': classifier.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                }\n",
    "                torch.save(state, savepath)\n",
    "                log_string('Saving model....')\n",
    "            log_string('Best mIoU: %f' % best_iou)\n",
    "        global_epoch += 1\n",
    "        plot_and_save_metrics(training_mean_loss, training_accuracy, eval_mean_loss, eval_accuracy, experiment_dir)\n",
    "        save_metrics(training_mean_loss, training_accuracy, eval_mean_loss, eval_accuracy, experiment_dir)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    args = parse_args()\n",
    "    main(args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55acdec4-1b6a-483b-850e-c518f5b4b436",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu-2.1.0-cuda-12.1",
   "language": "python",
   "name": "pytorch-gpu-2.1.0-cuda-12.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
